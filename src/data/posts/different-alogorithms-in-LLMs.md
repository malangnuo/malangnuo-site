---
title: '浅析大语言模型中的SFT、RLHF、DPO、PPO及GRPO算法'
publishDate: '2025-5-11'
description: '简要介绍大语言模型中的SFT、RLHF、DPO、PPO及GRPO算法'
introText: '大语言模型（LLMs）的快速发展推动了自然语言处理领域的巨大进步，但如何将其输出与人类偏好对齐仍是核心挑战。监督微调（SFT）、人类反馈强化学习（RLHF）、直接偏好优化（DPO）、近端策略优化（PPO）及组相对策略优化（GRPO）等算法在此过程中扮演了关键角色。本文将从技术原理、应用场景及对比分析等角度对这些算法进行深入探讨。'
author: '말랑누오'
tags: ['LLM', 'Algorithms', 'Reinforcement Learning', 'Supervised Learning']
slug: 'different-alogorithms-in-LLMs'
---
# 浅析大语言模型中的SFT、RLHF、DPO、PPO及GRPO算法

## 引言
大语言模型（LLMs）的快速发展推动了自然语言处理领域的巨大进步，但如何将其输出与人类偏好对齐仍是核心挑战。监督微调（SFT）、人类反馈强化学习（RLHF）、直接偏好优化（DPO）、近端策略优化（PPO）及组相对策略优化（GRPO）等算法在此过程中扮演了关键角色。本文将从技术原理、应用场景及对比分析等角度对这些算法进行深入探讨。

## 核心算法解析

### 监督微调（Supervised Fine-Tuning, SFT）
SFT是在预训练模型基础上，利用标注的（指令、答案）对进行有监督训练的方法。其核心目标是使模型适应特定任务，例如在GPT-3基础上通过SFT生成符合人类指令的响应。具体流程如下：
1. **数据准备**：构建包含输入指令和对应标准答案的数据集。
2. **模型微调**：通过交叉熵损失函数优化模型参数，公式为：
$L_{SFT} = -\frac{1}{n}\sum_{i=1}^n \log P(y_i|x_{<i})$
其中，$x_{<i}$为当前token之前的所有输入序列，$y_i$为目标输出token。

**优势与局限**：
- **优势**：简单直接，适用于有明确标注数据的任务，如文本分类和机器翻译。
- **局限**：依赖高质量标注数据，缺乏负反馈机制，且无法“向后看”整个生成序列，导致泛化能力有限。

### 人类反馈强化学习（Reinforcement Learning from Human Feedback, RLHF）
RLHF通过引入人类反馈作为奖励信号，进一步优化模型生成质量，是ChatGPT等模型的关键技术。其核心步骤包括：
1. **奖励模型训练**：基于人类对模型生成结果的排序数据，训练奖励模型（RM）预测人类偏好评分。
2. **策略优化**：使用PPO算法，以奖励模型输出为标量奖励，优化策略网络参数，公式为：
$L_{RLHF} = -\mathbb{E}_{x\sim\pi_\theta}[R(x)]$
其中，$R(x)$为奖励模型对序列$x$的评分，$\pi_\theta$为策略网络。

**技术特点**：
- **引入负反馈**：通过奖励模型对低质量输出进行惩罚，提升生成内容的安全性和相关性。
- **全局优化**：奖励模型可评估整个生成序列，解决了SFT的“向后看”问题。

### 直接偏好优化（Direct Preference Optimization, DPO）
DPO是斯坦福大学提出的新型优化方法，直接利用偏好数据优化模型，无需显式训练奖励模型。其核心思想是最大化偏好响应的相对概率，公式为：
$L_{DPO} = -\mathbb{E}_{(x,y_w,y_l)\sim D} \left[ \log \frac{\pi_\theta(y_w|x)}{\pi_{ref}(y_w|x)} - \log \frac{\pi_\theta(y_l|x)}{\pi_{ref}(y_l|x)} \right]$
其中，$y_w$和$y_l$分别为偏好和非偏好响应，$\pi_{ref}$为参考模型。

**优势分析**：
- **简化流程**：跳过奖励模型训练，直接通过分类损失优化策略，降低计算成本和训练复杂度。
- **稳定性提升**：避免了RLHF中奖励模型训练的不稳定性，在情感控制、摘要生成等任务中表现优异。

### 近端策略优化（Proximal Policy Optimization, PPO）
PPO是RLHF中常用的策略优化算法，通过限制策略更新步长确保训练稳定性。其核心机制包括：
1. **优势估计**：使用广义优势估计（GAE）结合短期和长期奖励信号。
2. **剪裁目标函数**：通过剪裁概率比约束策略更新幅度，公式为：
$L_{PPO} = \mathbb{E} \left[ \min \left( r_t(\theta)\hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t \right) \right]$
其中，$r_t(\theta)$为新旧策略的概率比，$\hat{A}_t$为优势估计值。

### 组相对策略优化（Group Relative Policy Optimization, GRPO）
GRPO是DeepSeek提出的新型RL算法，专为大语言模型设计，通过组内相对比较替代绝对价值估计。其核心创新点包括：
1. **无评论家网络**：去除PPO中的价值网络，节省内存并提高计算效率。
2. **长链推理支持**：通过组内对比学习，增强模型在数学推理、代码生成等复杂任务中的表现。

## 算法对比与选择

### 技术特性对比
| 算法   | 反馈类型       | 奖励模型需求 | 优化方式       | 计算成本 | 适用场景                |
|--------|----------------|--------------|----------------|----------|-------------------------|
| SFT    | 标注数据       | 无           | 监督学习       | 低       | 简单任务，标注数据充足  |
| RLHF   | 人类偏好排序   | 有           | 强化学习       | 高       | 复杂任务，需全局优化    |
| DPO    | 偏好对比数据   | 无           | 直接偏好优化   | 中       | 需高效对齐人类偏好      |
| PPO    | 标量奖励       | 有           | 策略梯度       | 高       | 策略优化，如RLHF流程    |
| GRPO   | 组内相对评分   | 无           | 组内对比学习   | 中       | 长链推理，多模态任务    |

### 关键差异分析
- **反馈机制**：SFT依赖显式标注，RLHF和DPO基于人类偏好，GRPO通过组内比较隐式建模偏好。
- **计算效率**：DPO和GRPO通过简化流程降低计算成本，而RLHF和PPO因涉及奖励模型和复杂优化步骤成本较高。
- **泛化能力**：RLHF和GRPO在复杂推理任务中表现更优，而SFT和DPO在特定领域任务中更具优势。

## 未来展望
大语言模型的对齐技术仍在快速演进，未来需在算法效率、泛化能力及伦理安全等方面持续突破，以实现更可靠的人工智能系统。
