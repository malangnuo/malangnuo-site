---
title: 'test'
publishDate: '2025-9-11'
description: '보들말랑누오'
introText: '
| 방법 | 구현 방식 | 장점 | 단점 |
| 누오는 말랑말랑하다 |'
author: '말랑누오'
tags: ['LLM', 'Algorithms']
slug: 'llm-reasoning-enhancement-methods'
---

# 누오토오누오토오


누누누누누누누 <br>
누오는 귀엽다
| 方法 | 做法 | 优点 | 缺点 |
| --- | --- | --- | --- |
| 知识蒸馏 (Knowledge Distillation) | 将大型复杂模型的推理模式迁移到轻量化的模型中，主要通过教师-学生架构进行训练，通常通过软目标（soft targets）或监督微调（SFT）来实现。 | 推理速度与学生模型架构相同，有效压缩模型，适合资源受限的场景，如移动端部署等 | 蒸馏后的模型性能存在一定损失；需要依赖教师模型的输出进行训练 |
| 数据集增强 (Data Augmentation) | 引入领域知识或结构化信息，对现有数据进行修改或合成，生成更高质量的训练样本 | 通过对数据集的多样化处理，增加训练集的多样性和规模，有效防止模型过拟合，提高模型的泛化能力和鲁棒性 | 依赖高质量数据，否则可能引入噪声；过程需要较高的计算资源和时间成本 |
| 思维链 (Chain of Thought) | 通过向模型展示推理过程的中间步骤，引导其生成推理的中间步骤，进而生成更加合理的答案 | 能显著提升模型在复杂推理任务（如数学、逻辑）上的表现；增强模型的透明度和可解释性 | 生成多步推理需要更多的计算资源和时间；有时会产生看似合理但错误的推理路径，影响最终结果的准确性 |
| 强化学习 (Reinforcement Learning) | 通过设计奖励函数，最大化奖励信号，来优化模型推理策略 | 通过设计奖励函数，最大化奖励信号，从而优化模型推理策略，能提升模型与人类价值观的一致性，增强安全性和实用性 | 训练过程计算成本高昂且复杂；设计一个有效的奖励函数非常困难，可能导致模型行为异常 |
| 检索增强生成 (Retrieval-Augmented Generation) | 模型根据用户查询，从外部知识库中检索相关文档，将检索到的文档作为附加上下文，增强生成回答的准确性 | 减少模型“幻觉”现象，使得回答更具事实性；能提供实时、最新的信息；允许模型引用来源 | 检索结果的质量直接影响回答质量，如果检索不准确则效果不佳；需要额外的知识库构建和维护 |
| 自洽性 (Self-Consistency) | 让模型对同一问题生成多个不同的推理路径和答案，然后从这些结果中选择出现频率最高或最一致的答案作为最终输出 | 显著提升复杂推理任务的准确率；比单一的CoT推理更加稳健 | 需要生成多条推理路径，会显著增加计算成本和推理延迟，特别是在大规模模型中可能会产生效率瓶颈 |
| 提示词工程 (Prompt Engineering) | 精心设计和优化给模型的指令和问题 | 无需改变模型内部参数；决定模型输出质量 | 提示词设计需要技巧和经验；不合适的提示词设计可能导致模型输出不一致或不相关的结果 |
| 上下文学习 (In-context Learning) | 在提示词中直接提供少量任务范例，让模型从这些例子中学习 | 无需微调模型，通过提供少量任务范例，模型便可根据示例从上下文中推断任务模式和风格，操作简便 | 效果依赖于范例的质量和数量；可能无法处理过于复杂的任务 |
| 上下文工程 (Context Engineering) | 系统性地构建和组织输入给模型的所有信息，如指令、范例、背景知识等。 | 系统性地构建和组织输入给模型的所有信息，包括指令、范例、背景知识等，确保模型获得完成复杂任务所需的全面信息 | 构建和管理上下文比较复杂；模型上下文窗口大小有限 |

Knowledge Distilling:

[Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531)

Data Augmentation:

[Data Augmentation using Large Language Models: Data Perspectives,...](https://arxiv.org/abs/2403.02990)

Chain of Thought: 

[Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)

Reinforcement Learning(LoRA):

[LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)

Retrieval-Augmented Generation:

[Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401)

Self-Consistency:

[Self-Consistency Improves Chain of Thought Reasoning in Language Models](https://arxiv.org/abs/2203.11171)

Prompt Engineering:

[gptaiflow.com](https://gptaiflow.com/assets/files/2025-01-18-pdf-1-TechAI-Goolge-whitepaper_Prompt%20Engineering_v4-af36dcc7a49bb7269a58b1c9b89a8ae1.pdf)

In-context Learning:

[A Survey on In-context Learning](https://arxiv.org/abs/2301.00234)

Context Engineering:

[A Survey of Context Engineering for Large Language Models](https://arxiv.org/abs/2507.13334v2)